{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67affd21",
   "metadata": {},
   "source": [
    "# Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27806e94",
   "metadata": {},
   "source": [
    "You may have come across Bayes theorem below (think school, venn diagrams and rearranging formulae for conditional probability - if you haven't / don't remember it, wikipedia it!):\n",
    "\n",
    "$$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Changing A and B into things we care about:\n",
    "\n",
    "$$P(model | data) = \\frac{P(model)P(data | model)}{P(data)}$$\n",
    "\n",
    "The names assigned to the parts of the equations are this:\n",
    "\n",
    "$$Posterior = \\frac{Prior~\\times~Likelihood}{Evidence}$$\n",
    "\n",
    "\n",
    "\n",
    "We usually want to know what is the probability that our model is correct given the data - this is what is on the left hand side. It's name is the 'posterior' distribution.\n",
    "\n",
    "But this can be hard to find...why?\n",
    "\n",
    "Well... $P(model)$ (called the prior) is easy: this is just the prior information we are choosing to include.\n",
    "\n",
    "$P(data | model)$ (called the likelihood) is usually not too hard to get the shape of the distribution. However, if we are doing this naively and evaluating the data given the model on a full grid of all possible parameters it might take forever! For example, if we have a 5 parameter model and we were to evaluate the likelihood at each of 10 values we will already have $10^{5}$ grid points!\n",
    "\n",
    "But $P(data)$ (the evidence) is usually pretty difficult! To calculate $P(data)$ you need to have the $P(data | \\textit{every hypothesis})$, this could take infinite values depending on your situation. But $P(data)$ doesn't depend on the model parameters itself. So if we can work out the shape of the final distribution we could then sample from it and use our samples to work out the probabilities.\n",
    "\n",
    "MCMC provides us a way to sample from the distribution by creating a chain where the 'stationary' or equilibrium distribution is the same shape as the posterior distribution, but we don't need to know the exact height of this distribution.\n",
    "\n",
    "The goal is to construct a sequence which walks around in such a way that the chain itself will have the desired stationary distribution.\n",
    "\n",
    "One algorithm is the $\\textit{metropolis hastings}$ algorithm. To demonstrate this, we'll revist fitting lines in noisy data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
